{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e1b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174b922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned_adult-all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68431797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of the target column\n",
    "df['gross_income'] = df['gross_income'].map({'>50K':1,'<=50K':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8257c459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "gross_income",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "bcc127b5-e7d4-4229-9884-c91f8821a2c3",
       "rows": [
        [
         "0",
         "37109"
        ],
        [
         "1",
         "11681"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "gross_income\n",
       "0    37109\n",
       "1    11681\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gross income (target) column\n",
    "df['gross_income'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4781ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['gross_income'] #target column\n",
    "x = df.drop(columns=['gross_income','fnlwgt','education_num']) #prepare the training dataset - dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a92b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold,cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler,PolynomialFeatures,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import TargetEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee450226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the metrics\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "def metrics(y_pred,y_test):\n",
    "    PrecisionScore = precision_score(y_pred,y_test)\n",
    "    RecallScore = recall_score(y_pred,y_test)\n",
    "    F1Score = f1_score(y_pred,y_test)\n",
    "    AccuracyScore = accuracy_score(y_pred,y_test)\n",
    "    return PrecisionScore,RecallScore,F1Score,AccuracyScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10dc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "    x,y,test_size=0.2,random_state=42 # split the dataset into four different sets\n",
    ")\n",
    "x_train.to_parquet('data/x_train.parquet',index=False)\n",
    "x_test.to_parquet('data/x_test.parquet',index=False)\n",
    "y_train.to_frame('y_train').to_parquet('data/y_train.parquet',index=False)\n",
    "y_test.to_frame('y_test').to_parquet('data/y_test.parquet',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e1ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = x_train.select_dtypes(include='number').columns.tolist() #isolate numeric columns \n",
    "# we further divide the categorical columns into high cardinality and low cardinality columns\n",
    "high_card_cols = ['education','occupation','workclass','native_country','marital_status']\n",
    "low_card_cols = ['sex','relationship','race'] \n",
    "\n",
    "# numerical data pipeline - scaling numerical values, and introducing polynomial features\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('scaler',RobustScaler()),\n",
    "    ('poly',PolynomialFeatures(include_bias=False)),\n",
    "])\n",
    "# high cardinality pipeline - use simpleImputer function to impute missing values, replacing missing values with the most frequent\n",
    "# occurence of a value\n",
    "# and the values are target encoded\n",
    "high_card_pipe = Pipeline(steps=[\n",
    "    ('impute',SimpleImputer(strategy='most_frequent')),\n",
    "    ('target',TargetEncoder()),\n",
    "])\n",
    "#low cardinality pipeline - onehot encode values\n",
    "low_card_pipe = Pipeline(steps=[\n",
    "    ('onehot',OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# project the changes we have made to respective columns using ColumnTransformer \n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num',num_pipe,numeric_cols),\n",
    "    ('low_card',low_card_pipe,low_card_cols),\n",
    "    ('high_card',high_card_pipe,high_card_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of several models and their respective parameters\n",
    "model_and_grid_params = {\n",
    "    #Logistic Regression\n",
    "        'Logistic Regression' : {\n",
    "        'model': LogisticRegression(penalty='l2',solver='lbfgs',n_jobs=-1,verbose=2),\n",
    "        'params' : {\n",
    "            \"preprocessor__num__poly__degree\" : [1,2],\n",
    "            \"classifier__C\": [0.1,1.0,10.0],\n",
    "            \"classifier__max_iter\" : [1000,2000,3000]\n",
    "        }\n",
    "    },\n",
    "    #Decision Trees\n",
    "    'Decision Trees' : {\n",
    "        'model' : DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            \"preprocessor__num__poly__degree\" : [1,2],\n",
    "            'classifier__max_depth' : [5,10,None],\n",
    "            'classifier__min_samples_split' : [100,150,200]\n",
    "        }\n",
    "    },\n",
    "    #Random Forest classifier\n",
    "    'Random Forest Classifier' : {\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            \"preprocessor__num__poly__degree\" : [1,2],\n",
    "            'classifier__n_estimators' : [80,100,120],\n",
    "            'classifier__max_depth' : [5,10,None],\n",
    "            'classifier__min_samples_split' : [100,150,200]\n",
    "        }\n",
    "    },\n",
    "    #XGB Classifier\n",
    "    'XGB Classifier' : {\n",
    "        'model' : XGBClassifier(objective='binary:logistic',verbosity=1,random_state=42),\n",
    "        'params' : {\n",
    "            \"preprocessor__num__poly__degree\" : [1,2],\n",
    "            'classifier__n_estimators' : [80,100,120],\n",
    "            'classifier__learning_rate' : [0.1,0.5,1.0],\n",
    "            'classifier__max_depth' : [3,5,8],\n",
    "            'classifier__reg_lambda' : [0.1,0.5,1,10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#cross validation - splitting training data into 5 folds\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "results = {}\n",
    "\n",
    "best_score = -float('inf')\n",
    "best_model_name = None\n",
    "best_estimator = None\n",
    "\n",
    "for name,model_grid in model_and_grid_params.items():\n",
    "    print(f'--Training {name}. This May Take A While...')\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor',preprocessor),\n",
    "        ('classifier',model_grid['model'])\n",
    "    ])\n",
    "\n",
    "    model = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=model_grid['params'],\n",
    "        cv = cv,\n",
    "        refit= True,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    #Training\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    #Prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "    # y_pred = (y_pred_proba >= 0.5).astype(int) # set decision threshold to 0.5\n",
    "    \n",
    "    PrecisionScore,RecallScore,F1Score,AccuracyScore = metrics(y_pred,y_test)\n",
    "\n",
    "    # store results in a dictionary\n",
    "    results[name] = {\n",
    "        'Best score' : model.best_score_,\n",
    "        'Best params' : model.best_params_,\n",
    "        'Precision_score' : PrecisionScore,\n",
    "        'recall_score' : RecallScore,\n",
    "        'f1_score' : F1Score,\n",
    "        'accuracy_score' : AccuracyScore\n",
    "    }\n",
    "    if model.best_score_ > best_score:\n",
    "        best_score = model.best_score_\n",
    "        best_model_name = name\n",
    "        best_estimator = model.best_estimator_ \n",
    "\n",
    "    # Save best model to joblib\n",
    "import joblib\n",
    "joblib.dump(model.best_estimator_,f'models/{name}_best_model.pkl')\n",
    "print(f'Saved {name} best model to models/',name,'_best_model.pkl')\n",
    "\n",
    "print('*'*50)\n",
    "    \n",
    "# Save summary of all results\n",
    "import json\n",
    "with open('models/model_results.json','w') as file:\n",
    "    json.dump(results,file,indent=4)\n",
    "    print('File has been successfully saved to models/model_result.json')\n",
    "\n",
    "# display results\n",
    "for name,result in results.items():\n",
    "    print(f'Model Name : {name}')\n",
    "    print('Best CV score : ',result['Best score'])\n",
    "    print('Best Params : ',result['Best params'])\n",
    "    print('Precision Score',result['Precision_score'])\n",
    "    print('Recall Score: ',result['recall_score'])\n",
    "    print('f1_score : ',result['f1_score'])\n",
    "    print('Accuracy Score : ',result['accuracy_score'])\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1873698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Neural Network to data (This May Take a While)\n",
      "976/976 - 11s - 11ms/step - accuracy: 0.7658 - loss: 19102.2871 - val_accuracy: 0.7641 - val_loss: 0.3671\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Accuracy Score :  0.7545603607296577\n"
     ]
    }
   ],
   "source": [
    "# neural network training\n",
    "x_train = preprocessor.fit_transform(x_train,y_train)\n",
    "x_test = preprocessor.transform(x_test)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Input,Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(x_train.shape[1],)), # input shape\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=7,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=1,activation=None)\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    loss=BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(learning_rate=0.03),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Training\n",
    "print('Fitting Neural Network to data (This May Take a While)')\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size= 32,\n",
    "    verbose= 2\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = (y_pred_prob>=0.5).astype(int).flatten()\n",
    "\n",
    "# Metric\n",
    "print('Accuracy Score : ',accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82717d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... \n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Successfully saved model\n"
     ]
    }
   ],
   "source": [
    "# training the best model again, this time, one-hot encoding all categorical features, to help with SHAP interpretability\n",
    "numeric_features = x_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = x_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\",   # drop columns not listed\n",
    "    verbose_feature_names_out=False  # <- Keeps clean names (no _x mess)\n",
    ")\n",
    "\n",
    "#cross validation - splitting training data into 5 folds\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "# --------------------\n",
    "# Build Pipeline\n",
    "# --------------------\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_grid = {\n",
    "        'classifier__n_estimators' : [80,100,120],\n",
    "        'classifier__learning_rate' : [0.1,0.5,1.0],\n",
    "        'classifier__max_depth' : [3,5,8],\n",
    "        'classifier__reg_lambda' : [0.1,0.5,1,10]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        estimator = clf,\n",
    "        param_grid = params_grid,\n",
    "        cv = cv,\n",
    "        refit= True,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "# --------------------\n",
    "# Train\n",
    "# --------------------\n",
    "print('Training ... ')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "best_model = model.best_estimator_\n",
    "# --------------------\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model,'models/XGBClassifer_onehot_model.pkl')\n",
    "print('Successfully saved model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
